#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨å£°éŸ³è¯†åˆ«ç³»ç»Ÿçš„å„ä¸ªç»„ä»¶
"""

import sys
from pathlib import Path

import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from src.audio_processor import AudioProcessor
from src.data_augmentation import AudioAugmentor
from src.model import create_model
from src.utils import setup_project_logging
from config.config import AUDIO_CONFIG, MODEL_CONFIG


def example_audio_processing():
    """
    ç¤ºä¾‹ï¼šéŸ³é¢‘å¤„ç†
    """
    print("=== éŸ³é¢‘å¤„ç†ç¤ºä¾‹ ===")
    
    # åˆ›å»ºéŸ³é¢‘å¤„ç†å™¨
    processor = AudioProcessor()
    
    # å‡è®¾æœ‰ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
    audio_file = Path("data/positive_samples/example.wav")
    
    if audio_file.exists():
        print(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_file}")
        
        # åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘
        features = processor.extract_features(audio_file)
        print(f"æå–çš„ç‰¹å¾å½¢çŠ¶: {features.shape}")
        
        # æå–ä¸åŒç±»å‹çš„ç‰¹å¾
        mel_spec = processor.extract_mel_spectrogram(audio_file)
        mfcc = processor.extract_mfcc(audio_file)
        spectral = processor.extract_spectral_features(audio_file)
        
        print(f"æ¢…å°”é¢‘è°±å›¾å½¢çŠ¶: {mel_spec.shape}")
        print(f"MFCCç‰¹å¾å½¢çŠ¶: {mfcc.shape}")
        print(f"é¢‘è°±ç‰¹å¾å½¢çŠ¶: {spectral.shape}")
    else:
        print(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        print("è¯·å…ˆå‡†å¤‡è®­ç»ƒæ•°æ®")


def example_data_augmentation():
    """
    ç¤ºä¾‹ï¼šæ•°æ®å¢å¼º
    """
    print("\n=== æ•°æ®å¢å¼ºç¤ºä¾‹ ===")
    
    # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
    augmentor = AudioAugmentor()
    
    # ç”Ÿæˆç¤ºä¾‹éŸ³é¢‘æ•°æ®
    sample_rate = AUDIO_CONFIG['sample_rate']
    duration = 2.0
    t = np.linspace(0, duration, int(sample_rate * duration))
    
    # ç”Ÿæˆæ­£å¼¦æ³¢ä½œä¸ºç¤ºä¾‹
    frequency = 440  # A4éŸ³ç¬¦
    audio = 0.5 * np.sin(2 * np.pi * frequency * t)
    
    print(f"åŸå§‹éŸ³é¢‘é•¿åº¦: {len(audio)} æ ·æœ¬")
    
    # åº”ç”¨ä¸åŒçš„å¢å¼ºæŠ€æœ¯
    augmented_audio = augmentor.time_stretch(audio, rate=1.2)
    print(f"æ—¶é—´æ‹‰ä¼¸åé•¿åº¦: {len(augmented_audio)} æ ·æœ¬")
    
    pitch_shifted = augmentor.pitch_shift(audio, n_steps=2)
    print(f"éŸ³è°ƒå˜æ¢åé•¿åº¦: {len(pitch_shifted)} æ ·æœ¬")
    
    # æ·»åŠ å™ªéŸ³
    noise_level = 0.01
    noisy_audio = augmentor.add_noise(audio, noise_level=noise_level)
    print(f"æ·»åŠ å™ªéŸ³åéŸ³é¢‘èŒƒå›´: [{np.min(noisy_audio):.3f}, {np.max(noisy_audio):.3f}]")
    
    # éŸ³é‡è°ƒèŠ‚
    volume_factor = 0.8
    volume_adjusted = augmentor.adjust_volume(audio, factor=volume_factor)
    print(f"éŸ³é‡è°ƒèŠ‚åæœ€å¤§å€¼: {np.max(np.abs(volume_adjusted)):.3f}")


def example_model_creation():
    """
    ç¤ºä¾‹ï¼šæ¨¡å‹åˆ›å»º
    """
    print("\n=== æ¨¡å‹åˆ›å»ºç¤ºä¾‹ ===")
    
    # å®šä¹‰è¾“å…¥å½¢çŠ¶ï¼ˆåŸºäºæ¢…å°”é¢‘è°±å›¾ï¼‰
    input_shape = (
        AUDIO_CONFIG['n_mels'],
        int(AUDIO_CONFIG['sample_rate'] * AUDIO_CONFIG['duration'] / AUDIO_CONFIG['hop_length']) + 1,
        1
    )
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_shape}")
    
    # åˆ›å»ºä¸åŒç±»å‹çš„æ¨¡å‹
    models = ['cnn', 'mobilenet', 'resnet']
    
    for model_type in models:
        print(f"\nåˆ›å»º {model_type.upper()} æ¨¡å‹...")
        
        try:
            model = create_model(model_type, input_shape=input_shape)
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            total_params = model.count_params()
            trainable_params = sum([np.prod(v.get_shape()) for v in model.trainable_weights])
            
            print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"  æ¨¡å‹å±‚æ•°: {len(model.layers)}")
            
            # æµ‹è¯•æ¨¡å‹é¢„æµ‹
            dummy_input = np.random.random((1,) + input_shape)
            prediction = model.predict(dummy_input, verbose=0)
            print(f"  é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {prediction.shape}")
            print(f"  é¢„æµ‹å€¼: {prediction[0][0]:.4f}")
            
        except Exception as e:
            print(f"  åˆ›å»º {model_type} æ¨¡å‹æ—¶å‡ºé”™: {e}")


def example_feature_extraction_pipeline():
    """
    ç¤ºä¾‹ï¼šå®Œæ•´çš„ç‰¹å¾æå–æµæ°´çº¿
    """
    print("\n=== ç‰¹å¾æå–æµæ°´çº¿ç¤ºä¾‹ ===")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = AudioProcessor()
    
    # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
    data_dir = Path("data/positive_samples")
    
    if data_dir.exists():
        audio_files = list(data_dir.glob("*.wav")) + list(data_dir.glob("*.mp3"))
        
        if audio_files:
            print(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
            
            # å¤„ç†å‰å‡ ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
            sample_files = audio_files[:min(3, len(audio_files))]
            
            features_list = []
            labels_list = []
            
            for audio_file in sample_files:
                try:
                    print(f"å¤„ç†: {audio_file.name}")
                    
                    # æå–ç‰¹å¾
                    features = processor.extract_features(audio_file)
                    features_list.append(features)
                    labels_list.append(1)  # æ­£æ ·æœ¬æ ‡ç­¾
                    
                    print(f"  ç‰¹å¾å½¢çŠ¶: {features.shape}")
                    print(f"  ç‰¹å¾èŒƒå›´: [{np.min(features):.3f}, {np.max(features):.3f}]")
                    
                except Exception as e:
                    print(f"  å¤„ç†æ–‡ä»¶ {audio_file.name} æ—¶å‡ºé”™: {e}")
            
            if features_list:
                # åˆå¹¶ç‰¹å¾
                all_features = np.array(features_list)
                all_labels = np.array(labels_list)
                
                print(f"\næ‰¹é‡ç‰¹å¾å½¢çŠ¶: {all_features.shape}")
                print(f"æ ‡ç­¾å½¢çŠ¶: {all_labels.shape}")
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                print(f"ç‰¹å¾å‡å€¼: {np.mean(all_features):.4f}")
                print(f"ç‰¹å¾æ ‡å‡†å·®: {np.std(all_features):.4f}")
        else:
            print("æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
    else:
        print(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")


def example_configuration():
    """
    ç¤ºä¾‹ï¼šé…ç½®ä½¿ç”¨
    """
    print("\n=== é…ç½®ç¤ºä¾‹ ===")
    
    print("éŸ³é¢‘é…ç½®:")
    for key, value in AUDIO_CONFIG.items():
        print(f"  {key}: {value}")
    
    print("\næ¨¡å‹é…ç½®:")
    for key, value in MODEL_CONFIG.items():
        print(f"  {key}: {value}")
    
    # å±•ç¤ºå¦‚ä½•ä¿®æ”¹é…ç½®
    print("\nä¿®æ”¹é…ç½®ç¤ºä¾‹:")
    print("# ä¸´æ—¶ä¿®æ”¹é‡‡æ ·ç‡")
    print("AUDIO_CONFIG['sample_rate'] = 16000")
    print("# ä¸´æ—¶ä¿®æ”¹æ‰¹æ¬¡å¤§å°")
    print("MODEL_CONFIG['batch_size'] = 64")


def example_logging():
    """
    ç¤ºä¾‹ï¼šæ—¥å¿—ä½¿ç”¨
    """
    print("\n=== æ—¥å¿—ç¤ºä¾‹ ===")
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_project_logging('INFO')
    
    # ä½¿ç”¨æ—¥å¿—
    logger.info("è¿™æ˜¯ä¸€æ¡ä¿¡æ¯æ—¥å¿—")
    logger.warning("è¿™æ˜¯ä¸€æ¡è­¦å‘Šæ—¥å¿—")
    logger.debug("è¿™æ˜¯ä¸€æ¡è°ƒè¯•æ—¥å¿—ï¼ˆå¯èƒ½ä¸ä¼šæ˜¾ç¤ºï¼‰")
    
    print("æ—¥å¿—å·²é…ç½®å®Œæˆ")


def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    """
    print("ğŸµ æ¸¸æˆå£°éŸ³è¯†åˆ«ç³»ç»Ÿ - åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    examples = [
        example_configuration,
        example_logging,
        example_audio_processing,
        example_data_augmentation,
        example_model_creation,
        example_feature_extraction_pipeline
    ]
    
    for example in examples:
        try:
            example()
        except Exception as e:
            print(f"è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 50)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ")
    print("\nğŸ’¡ æç¤º:")
    print("- å‡†å¤‡å¥½è®­ç»ƒæ•°æ®åï¼Œå¯ä»¥è¿è¡Œ python main.py train")
    print("- æŸ¥çœ‹æ›´å¤šé€‰é¡¹: python main.py --help")
    print("- å¿«é€Ÿæ£€æŸ¥ç¯å¢ƒ: python quick_start.py")


if __name__ == '__main__':
    main()